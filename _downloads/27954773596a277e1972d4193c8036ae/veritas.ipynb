{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# VERITAS with gammapy\n\nExplore VERITAS point-like DL3 files, including event lists and IRFs and\ncalculate Li & Ma significance, spectra, and fluxes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction\n\n[VERITAS](https://veritas.sao.arizona.edu/)_ (Very Energetic Radiation\nImaging Telescope Array System) is a ground-based gamma-ray instrument\noperating at the Fred Lawrence Whipple Observatory (FLWO) in southern\nArizona, USA. It is an array of four 12m optical reflectors for\ngamma-ray astronomy in the ~ 100 GeV to > 30 TeV energy range.\n\nVERITAS data are private and lower level analysis is done using either\nthe\n[Eventdisplay](https://github.com/VERITAS-Observatory/EventDisplay_v4)_\nor [VEGAS (internal access\nonly)](https://github.com/VERITAS-Observatory/VEGAS)_ analysis\npackages to produce DL3 files (using\n[V2DL3](https://github.com/VERITAS-Observatory/V2DL3)_), which can be\nused in Gammapy to produce high-level analysis products. A small sub-set\nof archival Crab nebula data has been publically released to accompany\nthis tutorial, which provide an introduction to VERITAS data analysis\nusing gammapy for VERITAS members and external users alike.\n\nThis notebook is only intended for use with these publically released\nCrab nebula files and the use of other sources or datasets may require\nmodifications to this notebook.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nfrom matplotlib import pyplot as plt\n\nimport astropy.units as u\n\nfrom regions import CircleSkyRegion\nimport matplotlib.pyplot as plt\nfrom gammapy.data import DataStore\nfrom gammapy.modeling.models import SkyModel, LogParabolaSpectralModel\nfrom gammapy.modeling import Fit\nfrom gammapy.datasets import Datasets, SpectrumDataset, FluxPointsDataset\nfrom gammapy.estimators import FluxPointsEstimator, LightCurveEstimator\nfrom gammapy.makers import (\n    ReflectedRegionsBackgroundMaker,\n    SafeMaskMaker,\n    SpectrumDatasetMaker,\n)\nfrom astropy.table import Table\nfrom astropy.coordinates import Angle, SkyCoord\nfrom gammapy.maps import MapAxis, RegionGeom, WcsGeom\nfrom gammapy.visualization import plot_spectrum_datasets_off_regions\nfrom astropy.time import Time\nfrom IPython.display import display"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load in files\n\nFirst, we select and load VERITAS observations of the Crab Nebula. These\nfiles are processed with **EventDisplay**, but VEGAS analysis should be\nidentical apart from the integration region size, which is specified in\nthe relevant section.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data_store = DataStore.from_dir(\"/home/feijen/veritas_data\")\ndata_store.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We filter our data by only taking observations within\n$5 \\deg$ of the Crab Nebula. See\n\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "target_position = SkyCoord(83.6333,22.0145,unit='deg')\n\nselection = dict(\n    type=\"sky_circle\",\n    frame=\"icrs\",\n    lon=f\"{target_position.ra.value} deg\",\n    lat=f\"{target_position.dec.value} deg\",\n    radius=\"5 deg\",\n)\nobs_table = data_store.obs_table.select_observations(selection)\nobs_ids = obs_table[\"OBS_ID\"]\nobservations = data_store.get_observations(obs_id=obs_ids,required_irf=\"point-like\")\n\nobs_ids = observations.ids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part I: Data Exploration\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Look at the information contained in the DL3 file for a single observation\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Peek at the IRFs included : point source files will contain effective\nareas, energy dispersion matrices, and events for both VEGAS and\nEventdisplay files. You should verify that the IRFs are filled correctly and\nthat there are no values set to zero within your analysis range.\n\nHere we peek at the first run in the data release: 64080. The Crab\nshould be visible in the events plot.\n\nYou can peek other runs by changing the index 0 to the appropriate\nindex.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "observations[0].peek(figsize=(25,5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Peek at the events and their time/energy/spatial distributions for run\n64080. We can also peek at the effective area (``aeff``) or energy migration\nmatrics (``edisp``) with the ``peek()`` method.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "observations[0].events.peek()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part II: Estimate counts and significance\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Set the energy binning\n\nThe energy axis will determine the bins in which energy is calculated,\nwhile the true energy axis defines the binning of the energy dispersion\nmatrix and effective area. Generally, the true energy axis should be more\nfinely binned than the energy axis and span a larger range of\nenergies, and the energy axis should be binned to match the needs of spectral\nreconstruction.\n\nNote that if the ``~gammapy.makers.SafeMaskMaker`` (which we will define later) is set\nto exclude events below a given percentage of the effective area, it will\nremove the entire bin containing the energy that corresponds to that\npercentage (which is why the energy axis below extends to broader\nenergies than the VERITAS energy sensitivity, in addition to catching\nevents for significance calculations that were mis-reconstructed to\nlow/high energies). Additionally, spectral bins are determined based on\nthe energy axis and cannot be finer or offset from the energy axis bin\nedges.\n\nDepending on your analysis requirements and your safe mask maker\ndefinition, ``energy_axis`` may need to be rebinned to ensure that the\nrequired energies are included. Note that finer binning will result in\nslower spectral/light curve calculations. See :doc:`/tutorials/api/makers.html#safe-data-range-handling`\nfor more information on how the safe mask maker works.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "energy_axis = MapAxis.from_energy_bounds(\"0.01 TeV\", \"100 TeV\", nbin=100)\nenergy_axis_true = MapAxis.from_energy_bounds(\n    \"0.01 TeV\", \"100 TeV\", nbin=200, name=\"energy_true\"\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create an exclusion mask\n\nHere, we create a spatial mask and append exclusion regions for the\nsource region, stars < 8th magnitude, and any other sources that we wish\nto exclude from background calculations.\n\nBy default, stars are excluded with a radius of 0.3 deg and sources with\na radius of 0.35 deg. This can be increased for brighter sources, as\nnecessary.\n\nTo exclude additional sources, more ``regions`` (of any geometry) can be\nappended to the ``all_ex`` list.\n\nHere, we use the Hipparcos catalog to search for bright stars within\n$1.75\\degree$ of our source - this radius can be changed in\n``star_mask``, additionally, to change from the default cut of 8th\nmagnitude, an additional masking condition can be used to decrease this\nthreshold - here we use 6th magnitude.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "exclusion_geom = WcsGeom.create(\n    skydir=(target_position.ra.value, target_position.dec.value),\n    binsz=0.01,\n    width=(4, 4),\n    frame=\"icrs\",\n    proj=\"CAR\",\n)\n\nregions = CircleSkyRegion(center=target_position, radius=0.35 * u.deg)\nall_ex = [regions]\n\nstar_data = np.loadtxt(\"/home/feijen/veritas_data/Hipparcos_MAG8_1997.dat\",usecols=(0, 1, 2, 3))\nstar_cat = Table(\n    {\n        \"ra\": star_data[:, 0],\n        \"dec\": star_data[:, 1],\n        \"id\": star_data[:, 2],\n        \"mag\": star_data[:, 3],\n    }\n)\nstar_mask = (\n    np.sqrt(\n        (star_cat[\"ra\"] - target_position.ra.deg) ** 2\n        + (star_cat[\"dec\"] - target_position.dec.deg) ** 2\n    ) < 1.75\n)\n\nfor src in star_cat[(star_mask) & (star_cat[\"mag\"] < 6)]:\n    all_ex.append(\n        CircleSkyRegion(\n            center=SkyCoord(src[\"ra\"], src[\"dec\"], unit=\"deg\", frame=\"icrs\"),\n            radius=0.3 * u.deg,\n        )\n    )\n\nexclusion_geom_image = exclusion_geom.to_image() # flatten the energy axis so that the exclusion mask is not energy dependent\nexclusion_mask = ~exclusion_geom_image.region_mask(all_ex)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define the integration region\n\nPoint-like DL3 files can only be analyzed using the reflected regions\nbackground method and for a pre-determined integration region (which is\nthe $\\sqrt{\\theta^2}$ used in IRF simulations), where the number\nof ON counts are determined.\n\nThe default values for moderate/medium cuts are as follows: \\* **For\nEventdisplay files (which applies to the files found in gammapy-data),\nthis ON region radius is $\\sqrt{0.008}\\degree$** \\* For VEGAS ITM\nfiles, this ON region radius is $\\sqrt{0.005}\\degree$ \\* For VEGAS\nGEO files, this ON region radius is $0.1 \\degree$\n\n*Note that full-enclosure files are required to use any other\nintegration radius size!*\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "on_region_radius = Angle(f\"{np.sqrt(0.008)} deg\")\non_region = CircleSkyRegion(center=target_position, radius=on_region_radius)\ngeom = RegionGeom.create(region=on_region, axes=[energy_axis])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SafeMaskMaker\n\nThe ``SafeMaskMaker`` sets the boundaries of our analysis based on the\nuncertainties contained in the instrument response functions (IRFs).\n\nFor VERITAS point-like analysis (both ED and VEGAS), the following\nmethods are strongly recommended: \\* ``offset-max``: Sets the maximum\nradial offset from the camera center within which we accept events. This\nis set to slightly below the edge of the VERITAS FoV to reduce artifacts\nat the edge of the FoV and events with poor angular reconstruction. \\*\n``edisp-bias``: Removes events which are reconstructed with energies\nthat have $>5\\%$ energy bias. \\* ``aeff-max``: Removes events\nwhich are reconstructed to $<10\\%$ of the maximum value of the\neffective area. These are important to remove for spectral analysis,\nsince they have large uncertainties on their reconstructed energies.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "safe_mask_maker = SafeMaskMaker(methods=[\"offset-max\",\"aeff-max\",\"edisp-bias\"], aeff_percent=5,bias_percent=5,offset_max=1.70*u.deg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will now run the data reduction chain to calculate our ON and OFF\ncounts. To get a significance for the whole energy range (to match VERITAS packages),\nremove the ``SafeMaskMaker`` from being applied to `dataset_on_off`.\n\nYou need to add ``containment_correction=True`` as an argument to\n``dataset_maker`` if you are using full-enclosure DL3 files.\n\nThe parameters of the reflected background regions can be changed using\nthe\n```ReflectedRegionsFinder`` :doc:`/tutorials/api/gammapy.makers.ReflectedRegionsFinder`\nwhich is passed as an argument to the\n``ReflectedRegionsBackgroundMaker``). To use the default values, do not\npass a region_finder argument.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dataset_maker = SpectrumDatasetMaker(\n    selection=[\"counts\", \"exposure\", \"edisp\"]\n)\ndataset_empty = SpectrumDataset.create(geom=geom, energy_axis_true=energy_axis_true)\nbkg_maker = ReflectedRegionsBackgroundMaker(exclusion_mask=exclusion_mask)\n\ndatasets = Datasets()\n\nfor obs_id, observation in zip(obs_ids, observations):\n    dataset = dataset_maker.run(dataset_empty.copy(name=str(obs_id)), observation)\n    dataset_on_off = safe_mask_maker.run(dataset, observation)\n    dataset_on_off = bkg_maker.run(dataset, observation)\n    datasets.append(dataset_on_off)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The plot below will show your exclusion regions in black and your\nbackground regions with coloured circles. You should check to make sure\nthese regions are sensible and that none of your background regions\noverlap with your exclusion regions.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(7,7))\nax = exclusion_mask.plot()\non_region.to_pixel(ax.wcs).plot(ax=ax, edgecolor=\"k\")\nplot_spectrum_datasets_off_regions(ax=ax, datasets=datasets)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Significance analysis results\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here, we display the results of the significance analysis.\n``info_table`` can be modified with ``cumulative = False`` to display a\ntable with rows that correspond to the values for each run separately.\n\nHowever, ``cumulative = True`` is needed to produce the combined values\nin the next cell.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "info_table = datasets.info_table(cumulative = True)\ninfo_table\n\nprint(f\"ON: {info_table['counts'][-1]}\")\nprint(f\"OFF: {info_table['counts_off'][-1]}\")\nprint(f\"Significance: {info_table['sqrt_ts'][-1]:.2f} sigma\")\nprint(f\"Alpha: {info_table['alpha'][-1]:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also plot the cumulative excess counts and significance over\ntime. For a steady source, we generally expect excess to increase\nlinearly with time and for significance to increase as\n$\\sqrt{\\textrm{time}}$.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, (ax_excess, ax_sqrt_ts) = plt.subplots(figsize=(10, 4), ncols=2, nrows=1)\nax_excess.plot(\n    info_table[\"livetime\"].to(\"h\"),\n    info_table[\"excess\"],\n    marker=\"o\",\n)\n\nax_excess.set_title(\"Excess\")\nax_excess.set_xlabel(\"Livetime [h]\")\nax_excess.set_ylabel(\"Excess events\")\n\nax_sqrt_ts.plot(\n    info_table[\"livetime\"].to(\"h\"),\n    info_table[\"sqrt_ts\"],\n    marker=\"o\",\n)\n\nax_sqrt_ts.set_title(\"Significance\")\nax_sqrt_ts.set_xlabel(\"Livetime [h]\")\nax_sqrt_ts.set_ylabel(\"Significance [sigma]\")\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part III: Make a spectrum\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, we\u2019ll calculate the source spectrum. This uses a forward-folding\napproach that will assume a given spectrum and fit the counts calculated\nabove to that spectrum in each energy bin specified by the\n``energy_axis``.\n\nFor this reason, it\u2019s important that ``spectral_model`` be set as\nclosely as possible to the expected spectrum - for the Crab nebula, this\nis a log parabola. If you don\u2019t know the spectrum a priori, this can be\nadjusted iteratively to get the best fit. Here, we are doing a 1D fit,\nso we assign only the spectral model to the datasets\u2019 ``SkyModel``\nbefore running the fit on our datasets.\n\nSee\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "spectral_model = LogParabolaSpectralModel(\n    amplitude=3.75e-11 * u.Unit(\"cm-2 s-1 TeV-1\"),\n    alpha=2.45,\n    beta=0.15,\n    reference=1 * u.TeV,\n)\n\nmodel = SkyModel(spectral_model=spectral_model, name=\"crab\")\n\ndatasets.models = [model]\n\nfit_joint = Fit()\nresult_joint = fit_joint.run(datasets=datasets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The best-fit spectral parameters are shown in this table.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display(datasets.models.to_parameters_table())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can inspect how well our data fit the model\u2019s predicted counts in\neach ``energy_axis`` bin.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ax_spectrum, ax_residuals = datasets[0].plot_fit()\nax_spectrum.set_ylim(0.1, 40)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now calculate flux points to get a spectrum by fitting the\n``result_joint`` model\u2019s amplitude in selected energy bands (defined by\n``energy_edges``). We set ``selection_optional = \"all\"`` in\n``FluxPointsEstimator``, which will include a calcuation for the upper\nlimits in bins with a significance $< 2\\sigma$.\n\nUpper limit and/or flux uncertainty calculations may not work for bins\nwithout a sufficient number of excess counts. To improve this, we can\nadd ``fpe.norm.min``, ``fpe.norm.max``, and ``fpe.norm.scan_values`` to\nincrease the range for which the norm (the normalization of the\npredicted signal counts; see orange curve above) is being fit. This will\nhelp avoid situations where the norm profiles are insufficient to\ncalculate a 2 sigma point or 95% C.L. upper limit.\n\nFlux points values can be viewed with ``flux_points.to_table()`` and/or\nsaved as an ascii or ecsv file with ``flux_points.write()``.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fpe = FluxPointsEstimator(\n    energy_edges = np.logspace(-0.7,1.5,12)*u.TeV,\n    source = \"crab\",\n    selection_optional = \"all\",\n)\nfpe.norm.min=-1e2\nfpe.norm.max=1e2\nfpe.norm.scan_values=np.array(np.linspace(-10,10,10))\nflux_points = fpe.run(datasets=datasets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, we can plot our flux points along with the best-fit spectral model.\nFor the Crab, curvature is clearly present in the spectrum and we can\nsee that the flux points closely follow the\n``LogParabolaSpectralModel``.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "flux_points_dataset = FluxPointsDataset(\n    data=flux_points, models=datasets.models)\n\nflux_points_dataset.plot_fit()\nplt.ylim(1e-20,)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part IV: Make a lightcurve and caluclate integral flux\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Integral flux can be calculated by integrating the spectral model we fit\nearlier. This will be a model-dependent flux estimate, so the choice of\nspectral model should match the data as closely as possible.\nAdditionally, sources with poor spectral fits due to low statistics may\nhave inaccurate flux estimations.\n\n``e_min`` and ``e_max`` should be adjusted depending on the analysis\nrequirements - different gamma/hadron cuts in Eventdisplay/VEGAS and/or\nobserving conditions will lead to different energy thresholds. Note that\nthe ``energy_axis`` lower bin edge containing ``e_min`` is the value\nused in the integral flux calculation, which is *not* necessarily\nidentical to the energy threshold the user defines.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "e_min = 0.25 * u.TeV\ne_max = 30 * u.TeV\n\n# flux,flux_err = result_joint.models[\"crab\"].spectral_model.integral_error(e_min,e_max)\n# print(f\"Integral flux > {e_min}: {flux.value:.2} +/- {flux_err.value:.2} {flux.unit}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we\u2019ll create a run-wise binned light curve. See the [light\ncurves for\nflares](https://docs.gammapy.org/1.3/tutorials/analysis-time/light_curve_flare.html)_\nfor instructions on how to set up sub-run binning. Here, we set our\nenergy edges so that the light curve has an energy threshold of 0.25 TeV\nand will plot upper limits for time bins with significance\n$<2 \\sigma$.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "lc_maker = LightCurveEstimator(\n    energy_edges=[0.25, 30] * u.TeV, source=\"crab\", reoptimize=False\n)\nlc_maker.n_sigma_ul = 2\nlc_maker.selection_optional = [\"ul\"]\nlc = lc_maker.run(datasets)\n\nfig, ax = plt.subplots(\n    figsize=(8, 6),\n)\nlc.sqrt_ts_threshold_ul = 2\nlc.plot(ax=ax, axis_name=\"time\",sed_type='flux')\n\n# these lines will print out a table with values for individual light curve bins\ntable = lc.to_table(format=\"lightcurve\", sed_type=\"flux\")\ndisplay(table[\"time_min\", \"time_max\", \"flux\", \"flux_err\"])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}